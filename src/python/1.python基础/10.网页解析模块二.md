---
footer: false
prev: false
next: false
---

# 10.网页解析模块二

## 1，re 正则

### 1、匹配单个字符与数字

| 匹配         | 说明                                                                                    |
| ------------ | --------------------------------------------------------------------------------------- |
| .            | 匹配除换行符以外的任意字符，当 flags 被设置为 re.S 时，可以匹配包含换行符以内的所有字符 |
| []           | 里面是字符集合，匹配[]里任意一个字符                                                    |
| [0123456789] | 匹配任意一个数字字符                                                                    |
| [0-9]        | 匹配任意一个数字字符                                                                    |
| [a-z]        | 匹配任意一个小写英文字母字符                                                            |
| [A-Z]        | 匹配任意一个大写英文字母字符                                                            |
| [A-Za-z]     | 匹配任意一个英文字母字符                                                                |
| [A-Za-z0-9]  | 匹配任意一个数字或英文字母字符                                                          |
| [^lucky]     | []里的^称为脱字符，表示非，匹配不在[]内的任意一个字符                                   |
| ^[lucky]     | 以[]中内的某一个字符作为开头                                                            |
| \d           | 匹配任意一个数字字符，相当于[0-9]                                                       |
| \D           | 匹配任意一个非数字字符，相当于`[^0-9]`                                                  |
| \w           | 匹配字母、下划线、数字中的任意一个字符，相当于[0-9A-Za-z_]                              |
| \W           | 匹配非字母、下划线、数字中的任意一个字符，相当于`[^0-9A-Za-z_]`                         |
| \s           | 匹配空白符(空格、换页、换行、回车、制表)，相当于[ \f\n\r\t]                             |
| \S           | 匹配非空白符(空格、换页、换行、回车、制表)，相当于`[^ \f\n\r\t]`                        |

### 2、匹配锚字符

锚字符:用来判定是否按照规定开始或者结尾

| 匹配 | 说明                            |
| ---- | ------------------------------- |
| ^    | 行首匹配，和[]里的^不是一个意思 |
| $    | 行尾匹配                        |

### 3、限定符

限定符用来指定正则表达式的一个给定组件必须要出现多少次才能满足匹配。有 \* 或 + 或 ? 或 {n} 或 {n,} 或 {n,m} 共 6 种。

| 匹配     | 说明                                                 |
| -------- | ---------------------------------------------------- |
| (xyz)    | 匹配括号内的 xyz，作为一个整体去匹配 一个单元 子存储 |
| x?       | 匹配 0 个或者 1 个 x，非贪婪匹配                     |
| x\*      | 匹配 0 个或任意多个 x                                |
| x+       | 匹配至少一个 x                                       |
| x\{n\}   | 确定匹配 n 个 x，n 是非负数                          |
| x\{n,\}  | 至少匹配 n 个 x                                      |
| x\{n,m\} | 匹配至少 n 个最多 m 个 x                             |
| x\|y     | \|表示或的意思，匹配 x 或 y                          |

**通用 flags（修正符）**

| 值   | 说明                            |
| ---- | ------------------------------- |
| re.I | 是匹配对大小写不敏感            |
| re.S | 使.匹配包括换行符在内的所有字符 |

^a 匹配 a 开头的

^[a] 匹配一个小写字母 a 并且 a 作为开头，等同于^a

[^a] 匹配一个小写字母 a 以外的任意字符

### 4，匹配符总结

```python
1  []原子表
[a]  匹配一个小写字母a
[1]  匹配一个数字1
[ab] 匹配一个小写字母a或者b
[a1] 匹配一个小写字母a或者数字1
[123] 匹配一个数字1或者2或者3
[a-z] 匹配任意一个小写字母
[A-Z] 匹配任意一个大写字母
[a-zA-Z]  匹配任意一个字母
[0-9] 匹配任意一个数字 0-9
[a-zA-Z0-9]  匹配任意一个数字 0-9或者任意一个字母
以上不管[]中有多少内容 只匹配一个
2 {m}  限定符 不能单独使用  限定前面那个正则的m次
a     匹配一个小写字母a
ab    匹配小写字母ab
aa     匹配2个小写字母a
a{2}     匹配2个小写字母a
ab{2}    匹配小写字母abb
a{2}b{2}    匹配小写字母aabb
[a-zA-Z]{5}  匹配任意5个字母

3 {m, n}  限定符 不能单独使用  限定前面那个正则的m-n次
[a-zA-Z]{3,5}  匹配任意3-5个字母

4 {m,}  限定符 不能单独使用  限定前面那个正则的m次
[a-zA-Z]{3,}  至少匹配任意3个字母

5 ^ 以...开始
abc    匹配abc三个字母
^abc    匹配以abc开头的三个字母
^[a]  匹配一个小写字母a并且a作为开头  等同于  ^a
[^a]  匹配一个小写字母a以外的任意字符


6 $   以...结尾
abc    匹配abc三个字母并且作为结尾
^abc     匹配以abc开头的三个字母
abc$    匹配abc三个字母并且作为结尾

7 ^$ 一般组合使用  (完全匹配)
匹配手机号
1[3-9][0-9]{9}
^1[3-9][0-9]{9}$

8 ?  匹配前面的正则0次或1次 相当于 {0,1}
-?[1-9] 匹配正负1-9

9 .  匹配换行符以外的任意字符

10 *  匹配任意次 {0,}

11  .*  匹配除换行符以外任意字符任意次 贪婪模式

12 .*?   匹配除换行符以外任意字符任意次 拒绝贪婪模式   (用的多)

13  +   匹配至少一次  相当于{1, }

14 .+    匹配除换行符以外任意字符至少1次 贪婪模式

15 .+?   匹配除换行符以外任意字符至少1次 拒绝贪婪模式

16  ()   子存储(会把括号里的值单独的保存下来)  一个单元 (ab)|(cd)

17  \w  匹配一位数字，字母，下划线  [a-zA-Z0-9_]

18  \W

19  \d  匹配一位数字   [0-9]

20  \D 和上面相反   [^0-9]


21  \s  匹配空白符

22  \S  和上面相反
```

### 5、贪婪与非贪婪

```python
#贪婪模式

#贪婪概念：匹配尽可能多的字符

# + .+  匹配换行符以外的字符至少一次
# + .*  匹配换行符以外的字符任意次

res = re.search('<b>.+</b>', '<b></b><b>b标签</b>')
res = re.search('<b>.*</b>', '<b>b标签</b><b>b标签</b><b>b标签</b><b>b标签</b>')

# .+?  匹配换行符以外的字符至少一次  拒绝贪婪
# + .*?   匹配换行符以外的字符任意次      拒绝贪婪

res = re.search('<b>.+?</b>', '<b>b标签</b><b>b标签</b>')
res = re.search('<b>.*?</b>', '<b>b标签</b><b>b标签</b><b>b标签</b><b>b标签</b>')
```

### 6，正则用到的方法

#### 1，re.search

```python
# re.search
# 返回第一个匹配的结果
res = re.search('a','abcdef343')    # 返回一个对象 <re.Match object; span=(0, 1), match='a'>
# 匹配上才可以用group拿结果，不然的话匹配不上返回None,不能用group，要不然会报错
print(res.group())  # a
```

#### 2，re.match

```python
res = re.match('\d{2}','123')
print(res.group())

#match函数
# match 必须第一位就开始匹配  否则匹配失败
# 给当前匹配到的结果起别名
s = '3G4HFD567'
x = re.match("(?P<value>\d+)",s)
print(x.group(0)) # 3
print(x.group('value')) # 3
```

#### 3，re.findall

```python
# findall
str = '<br>加粗1</br><br>加粗2</br><br>加粗3</br><br></br>'
res = re.findall('<br>.*?</br>',str)   # ['<br>加粗1</br>', '<br>加粗2</br>', '<br>加粗3</br>', '<br></br>']
res = re.findall('<br>.*</br>',str)   # ['<br>加粗1</br><br>加粗2</br><br>加粗3</br><br></br>']
res = re.findall('<br>.+?</br>',str)   # ['<br>加粗1</br>', '<br>加粗2</br>', '<br>加粗3</br>']
res = re.findall('<br>.+</br>',str)   # ['<br>加粗1</br><br>加粗2</br><br>加粗3</br><br></br>']

Str = '''
<a href="http://www.baidu.com">百度</a>
<A href="https://www.taobao.com">淘宝</A>
<a href="https://www.sina.com">新
浪</a>
'''

# 1，匹配出所有小写a的超链接
print(re.findall('<a href=".*?">.*?</a>',Str))
# ['<a href="http://www.baidu.com">百度</a>']

# .*? 匹配任意字符任意次，拒绝贪婪
print(re.findall('<a href=".*?">.*?</a>',Str,flags=re.S))
# ['<a href="http://www.baidu.com">百度</a>', '<a href="https://www.sina.com">新\n浪</a>']

# 2，匹配所有小写a或者大写A的超链接
print(re.findall('<[aA] href=".*?">.*?</[aA]>',Str,flags=re.S))
# ['<a href="http://www.baidu.com">百度</a>', '<A href="https://www.taobao.com">淘宝</A>', '<a href="https://www.sina.com">新\n浪</a>']

# 用 re.I 匹配大小写字符  re.S 可以匹配换行符
print(re.findall('<a href=".*?">.*?</a>',Str,flags=re.S | re.I))
# ['<a href="http://www.baidu.com">百度</a>', '<A href="https://www.taobao.com">淘宝</A>', '<a href="https://www.sina.com">新\n浪</a>']

# 3，获取网址和名称  （） 给谁加括号，谁就返回
print(re.findall('(<a href="(.*?)">(.*?)</a>)',Str,flags=re.S | re.I))
# [('<a href="http://www.baidu.com">百度</a>', 'http://www.baidu.com', '百度'), ('<A href="https://www.taobao.com">淘宝</A>', 'https://www.taobao.com', '淘宝'), ('<a href="https://www.sina.com">新\n浪</a>', 'https://www.sina.com', '新\n浪')]
```

#### 4，finditer

```Python
res = re.finditer('[a-z]','asdjgedksa43g')
print(res)  # 返回一个迭代器  <callable_iterator object at 0x0000015D8995F790>
print(next(res)) # <re.Match object; span=(0, 1), match='a'> 是一个对象，用group来取值
print(next(res).group()) # s

for i in res:
	print(i) # 返回一个迭代器  <callable_iterator object at 0x0000015D8995F790>
	print(i.group()) # 返回结果

```

#### 5，group 和 groups 区别

```python
# group 取第一个匹配的
print(re.search("<b>.*?</b>","<b>加粗</b>").group())  # <b>加粗</b>
print(re.search("<b>(?P<val>.*?)</b>","<b>加粗</b>").group())  # <b>加粗</b>
print(re.search("<b>(?P<val>.*?)</b>","<b>加粗</b>").group(0))  # <b>加粗</b>
print(re.search("<b>(?P<val>.*?)</b>","<b>加粗</b>").group(1))  # 加粗
# print(re.search("<b>(?P<val>.*?)</b>","<b>加粗</b>").group(2))  # 报错
print(re.search("<b>(?P<val>.*?)</b>","<b>加粗</b>").group('val'))  # 加粗

# goups 返回所有括号中的值
print(re.search("<a href='(.*?)'>(.*?)</a>","<a href='www.baidu.com'>百度</a>").groups()) # ('www.baidu.com', '百度')
```

#### 6，re.split 正则拆分

```python
print(re.split('\d','fdas3fedsa5fd45fdsa34fg4')) # ['fdas', 'fedsa', 'fd', '', 'fdsa', '', 'fg', '']
```

#### 7，re.sub 正则替换

```python
print(re.sub('\d','---','ab1fdsa456fdsa34fds65as35')) # ab---fdsa---------fdsa------fds------as------
```

#### 8，re.compile

```python
# compile函数
re_phone = re.compile(r"(0\d{2,3}-\d{7,8})")

s1 = "lucky's phone is 010-88888888"
s2 = "kaige's phone is 010-99999999"
ret1 = re_phone.search(s1)
ret2 = re_phone.search(s2)
```

## 2，xpath 解析

#### 2-1 导入使用

```python
#导入和使用
from lxml import etree
html_tree = etree.HTML(html字符串)
html_tree.xpath()
# 使用xpath路径查询信息，返回一个列表

from lxml import etree
# 第一种方式
parse = etree.HTMLParser(encoding='UTF-8')
tree = etree.parse('./素材/豆瓣.html', parser=parse)
print(tree)

# 第二种 推荐
data = open('./素材/豆瓣.html', 'r', encoding='UTF-8').read()
tree = etree.HTML(data)
print(tree)

```

#### 2-2 xpath 的使用

##### 2-2-1 特定路径匹配

```python
# 找登陆 获取当前路径下的所有匹配的a
a_list = tree.xpath('/html/body/div/div/div/a')


for a in a_list:
    print(a)  # <Element a at 0x2bdd868bc00>
    print(a.text)  # 登录 （获取文本）
    ## 想要把节点转换成看得懂的字符串标签数据
    print(etree.tostring(a, encoding='UTF-8').decode('UTF-8'))
    # <a href="https://www.douban.com/accounts/login?source=book" class="nav-login" rel="nofollow">登录</a>


a_list = tree.xpath('/html/body/div/div/div/a')
a_list = tree.xpath('/html/body/div/div/div/div/p/text()')
a_list = tree.xpath('/html/body/div/div/div[1]/a[1]/text()')
a_list = tree.xpath('/html/body/div/div/div//a/text()')
```

##### 2-2-2 获取当前路径下的文本

```Python
# 第一种
a_list = tree.xpath('/html/body/div/div/div/a/text()')

# 第二种
a_list = tree.xpath('/html/body/div/div/div/a')
for a in a_list:
    print(a.text)  #（获取文本）
```

##### 2-2-3 //

```python
# 不考虑当前所在位置

# 我想获取当前对象里所有的a
a_list = tree.xpath('//a')
a_list = tree.xpath('//a/text()')
```

##### 2-2-4 获取属性

```python
# 获取img的src属性值
img_src = tree.xpath('//ul/li/a/img/@src')
img_src = tree.xpath('//ul/li//a/img/@src')
```

##### 2-2-5 添加条件

```python
# 添加class条件
img_src = tree.xpath('//ul[@class="cover-col-4 clearfix"]/li/div/h2/a/text()')
img_src = tree.xpath('//ul[@class="cover-col-4 clearfix"]/li/div[@class="detail-frame"]/h2/a/text()')
```

##### 2-2-6 位置查找

```python
# 获取第一个li
li = tree.xpath('//ul[@class="cover-col-4 clearfix"]/li[1]/div/h2/a/text()')

# 获取第二个li
li = tree.xpath('//ul[@class="cover-col-4 clearfix"]/li[2]/div/h2/a/text()')

# 获取最后一个li
li = tree.xpath('//ul[@class="cover-col-4 clearfix"]/li[last()]/div/h2/a/text()')

# 获取倒数第二个li
li = tree.xpath('//ul[@class="cover-col-4 clearfix"]/li[last()-1]/div/h2/a/text()')

# 获取前俩个li
li = tree.xpath('//ul[@class="cover-col-4 clearfix"]/li[position()<3]/div/h2/a/text()')

# 可以使用列表切片解决啊
li = tree.xpath('//ul[@class="cover-col-4 clearfix"]/li/div/h2/a/text()')[0]
li = tree.xpath('//ul[@class="cover-col-4 clearfix"]/li/div/h2/a/text()')[0:2]
```

##### 2-2-7 ./ .//

```python
# 一个完整的xpath路径，但是可以拆分
li = tree.xpath('//ul[@class="cover-col-4 clearfix"]/li/div/h2/a/text()')

# 先匹配到li 再继续往下匹配
li_list = tree.xpath('//ul[@class="cover-col-4 clearfix"]/li')
for li in li_list:
    print(li) # xpath 对象
    # 从当前位置向下匹配
    print(li.xpath('./div/h2/a/text()'))
    print(li.xpath('.//div/h2/a/text()'))
```

##### 2-2-8 属性（一般用不到）

```Python
# 获取ul的class属性为cover-col-4 clearfix的ul下面的儿子li
li_list = tree.xpath('//ul[@class="cover-col-4 clearfix"]/li')

# 选取所有ul具有class属性的节点
li_list = tree.xpath('//ul[@class]')

# 获取所有ul具有aa属性的节点
li_list = tree.xpath('//ul[@aa]')

```

##### 2-2-9 多条件 and or |

```python
# 多个条件  and  or
print(tree.xpath('//div[@id="db-global-nav"]'))
print(tree.xpath('//div[@class="global-nav"]'))

# 获取同时满足id为db-global-nav class为global-nav 的ul
print(tree.xpath('//div[@class="global-nav" and @id="db-global-nav"]'))

# 获取满足id为db-global-nav 或 class为global-nav 的ul
print(tree.xpath('//div[@class="global-nav" or @id="db-global-nav"]'))

# |
print(tree.xpath('//div[@id="db-global-nav"] | //div[@class="global-nav"]'))
```

#### 2-3 xpath 语法

##### 2-3-1 路径表达式

| 路径表达式            | 结果                                                   |
| :-------------------- | :----------------------------------------------------- |
| /ul/li[1]             | 选取属于 ul 子元素的第一个 li 元素。                   |
| /ul/li[last()]        | 选取属于 ul 子元素的最后一个 li 元素。                 |
| /ul/li[last()-1]      | 选取属于 ul 子元素的倒数第二个 li 元素。               |
| //ul/li[position()<3] | 选取最前面的两个属于 ul 元素的子元素的 li 元素。       |
| //a[@title]           | 选取所有拥有名为 title 的属性的 a 元素。               |
| //a[@title='xx']      | 选取所有 a 元素，且这些元素拥有值为 xx 的 title 属性。 |

##### 2-3-2 选取未知节点

XPath 通配符可用来选取未知的 XML 元素。

| 通配符 | 描述                                                |
| :----- | :-------------------------------------------------- |
| \*     | 匹配任何元素节点。 一般用于浏览器 copy xpath 会出现 |
| @\*    | 匹配任何属性节点。                                  |
| node() | 匹配任何类型的节点。                                |

**实例**

在下面的表格中，我们列出了一些路径表达式，以及这些表达式的结果：

| 路径表达式  | 结果                            |
| :---------- | :------------------------------ |
| /ul/\*      | 选取 ul 元素的所有子元素。      |
| //\*        | 选取文档中的所有元素。          |
| //title[@*] | 选取所有带有属性的 title 元素。 |
| //node()    | 获取所有节点                    |

选取未知节点

| 路径表达式                       | 结果                                                                                |
| :------------------------------- | :---------------------------------------------------------------------------------- |
| //book/title \| //book/price     | 选取 book 元素的所有 title 和 price 元素。                                          |
| //title \| //price               | 选取文档中的所有 title 和 price 元素。                                              |
| /bookstore/book/title \| //price | 选取属于 bookstore 元素的 book 元素的所有 title 元素，以及文档中所有的 price 元素。 |

##### 2-3-3 逻辑运算

- 查找所有 id 属性等于 head 并且 class 属性等于 s_down 的 div 标签

  ```python
  //div[@id="head" and @class="s_down"]
  ```

- 选取文档中的所有 title 和 price 元素。

  ```python
  //title | //price
  ```

  注意: “|”两边必须是完整的 xpath 路径

##### 2-3-4 属性查询

- 查找所有包含 id 属性的 div 节点

  ```python
  //div[@id]
  ```

- 查找所有 id 属性等于 maincontent 的 div 标签

  ```python
  //div[@id="maincontent"]
  ```

- 查找所有的 class 属性

  ```python
  //@class
  ```

- //@attrName

  ```python
  //li[@name="xx"]//text()  # 获取li标签name为xx的里面的文本内容
  ```

* 获取第几个标签 索引从 1 开始

  ```python
  tree.xpath('//li[1]/a/text()')  # 获取第一个
  tree.xpath('//li[last()]/a/text()')  # 获取最后一个
  tree.xpath('//li[last()-1]/a/text()')  # 获取倒数第二个
  ```

##### 2-3-5 内容查询

- 查找所有 div 标签下的直接子节点 h1 的内容

  ```python
  //div/h1/text()
  ```

- 属性值获取

  ```python
  //div/a/@href   获取a里面的href属性值
  ```

- 获取所有

  ```python
  //*  #获取所有
  //*[@class="xx"]  #获取所有class为xx的标签
  ```

- 获取节点内容转换成字符串

  ```python
  c = tree.xpath('//li/a')[0]
  result=etree.tostring(c, encoding='utf-8')
  print(result.decode('UTF-8'))
  ```

## 3，bs4 解析

### 1，导入和使用

```Python
from bs4 import BeautifulSoup
soup = BeautifulSoup(html_doc, 'lxml')
# html进行美化
print(soup.prettify())

# 可以传入一段字符串或一个文件句柄.
from bs4 import BeautifulSoup
soup = BeautifulSoup(open("index.html"))
soup = BeautifulSoup("<html>data</html>", 'lxml')

# beautifulsoup  lxml
from bs4 import BeautifulSoup

f = open('./素材/豆瓣.html', 'r', encoding='UTF-8')
data = f.read()
# 第一种方式  建议使用这种
soup = BeautifulSoup(data, 'lxml')

# 第二种方式（）
soup = BeautifulSoup(open('./素材/豆瓣.html', 'r', encoding='UTF-8'), 'lxml')
print(soup)
print(type(soup))
```

### 2，浏览器结构化数据

#### 2-1 .语法 soup 对象.标签名

```Python
## .标签和.find 只获取第一个

soup.title  # 获取标签title
# <title>The Dormouse's story</title>

soup.title.name   # 获取标签名称
# 'title'

soup.title.string   # 获取标签title内的内容
# 'The Dormouse's story'

soup.title.parent  # 获取父级标签

soup.title.parent.name  # 获取父级标签名称
# 'head'


print(soup.title)
print(soup.div)
print(soup.a)
print(soup.img)
print(soup.abc)  # 不存在则为None

```

#### 2-2 soup.find

```python
# find  上面的.标签名 是当前find的简写   find可以给条件   .标签和.find 只获取第一个
print(soup.find('title'))
print(soup.find('div'))
# 获取soup对象中的第一个img标签
print(soup.img)
print(soup.find('img'))
```

#### 2-3 获取属性

```python
.语法或者find都只获取第一个

print(soup.div.attrs) # {'id': 'db-global-nav', 'class': ['global-nav']}
print(soup.div.attrs['id'])  # db-global-nav
print(soup.div.attrs['class']) # ['global-nav']
print(soup.div['id']) # db-global-nav
print(soup.div['class']) # ['global-nav']
```

#### 2-4 find 条件查找

```python
print(soup.find('a', class_="cover")) # 查找第一个class为cover的a标签
print(soup.find('p', class_="rating"))
print(soup.find('div', id="wrapper"))
print(soup.find('div', id="db-global-nav", class_="global-nav"))

## 可以用字典的形式查找满足多个属性的标签
print(soup.find('div', attrs={'id': "db-global-nav", 'class': "global-nav"}))

# class为多个的中间空格隔开就行
print(soup.find('ul', attrs={'class': "cover-col-4 clearfix"}))
print(soup.find('ul', attrs={'class': "caover-col-4 clearfix"}))
```

#### 2-5 find 和 .语法组合使用

```python
print(soup.find('a', class_="cover"))
print(type(soup.find('a', class_="cover")))  # <class 'bs4.element.Tag'> bs4对象才可以用组合使用
print(soup.find('a', class_="cover").find('img')) # 获取第一个a标签里的第一个img
print(soup.find('a', class_="cover").img) # 获取第一个a标签里的第一个img
print(soup.find('a', class_="cover").img.attrs) # {'src': 'https://img3.doubanio.com/mpic/s29535271.jpg'}
print(soup.find('a', class_="cover").img.attrs['src']) # https://img3.doubanio.com/mpic/s29535271.jpg
print(soup.find('a', class_="cover").img['src']) # https://img3.doubanio.com/mpic/s29535271.jpg
```

#### 2-6 写入到文件

```python
# 写入本地需要注意的点
with open('img.html', 'w', encoding='UTF-8') as f:
    f.write(str(soup.find('a', class_="cover").img))  ## 写入本地需要先转换为字符串，要不然会报错
    # f.write(soup.title.string)
```

#### 2-7 获取文本

```python
print(soup.title) # <title>新书速递</title>
print(soup.title.string) # 新书速递
print(type(soup.title.string)) # <class 'bs4.element.NavigableString'>
print(soup.title.strings)  # generator 对象
print(list(soup.title.strings))  # ['新书速递']

print(soup.title.text) # 新书速递
print(soup.title.get_text()) # 新书速递
print(soup.title.stripped_strings) # generator 对象
print(list(soup.title.stripped_strings)) # ['新书速递']
```

#### 2-8 多层嵌套标签

```python
## string 和 strings 区别
print(soup.find('div', class_="detail-frame"))
print(soup.find('div', class_="detail-frame").string)  # None
print(soup.find('div', class_="detail-frame").strings) # generator 对象
print(list(soup.find('div', class_="detail-frame").strings))  # 获取子子孙孙的文本 生成器返回

# text , get_text() , stripped_strings 区别
print(soup.find('div', class_="detail-frame").text)  # 返回所有文本字符串 包含非打印字符
print(soup.find('div', class_="detail-frame").get_text())  #和text一样 返回所有文本字符串 包含非打印字符
print(list(soup.find('div', class_="detail-frame").stripped_strings))  # 返回所有去除空白字符后的文本
```

#### 2-9 prettify 美化

```python
print(soup.find('div', class_="detail-frame"))
print(soup.find('div', class_="detail-frame").prettify())
```

#### 2-10 find_all

```python
# 查找所有 和find区别就是  查找所有 参数一样使用  find返回一个  find_all返回列表
print(soup.find_all('img'))   # 返回列表
print(soup.find_all('div', id="db-global-nav", class_="global-nav"))
print(soup.find_all('div', attrs={'id': "db-global-nav", 'class': "global-nav"}))
print(soup.find_all('div', limit=2))  # 取几个值  没啥用(我们用切片就完事了)
print(soup.find_all(['h2', 'img']))  # 获取h2和img标签
```

#### 2-11 select

```python
# select 查找所有  条件是选择器
print(soup.select('img'))
print(soup.select('.cover'))
print(soup.select('#db-global-nav'))
print(soup.select('.cover-col-4.clearfix'))
print(soup.select('.cover-col-4.clearfix#abc'))
print(soup.select('ul[class="cover-col-4 clearfix"]'))
print(soup.select('.cover-col-4.clearfix > li img'))
```
